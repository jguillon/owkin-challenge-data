{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning using Keras\n",
    "\n",
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeremy_damien_guillon/owkin-challenge-data\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd /home/jeremy_damien_guillon/owkin-challenge-data\n",
    "\n",
    "import owkin.utils as utils\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data properties:\n",
    "HEIGHT = 224 # our images height (in pixels)\n",
    "WIDTH = 224 # our images width (in pixels)\n",
    "CHANNELS = 3 # our images channels (i.e. RVB)\n",
    "DATA_DIR = 'data' # data directory where .csv files are stored\n",
    "IMAGES_DIR = 'data/train_input/images' # directory where images are stored\n",
    "TEST_IMAGES_DIR = 'data/test_input/images' # directory where images are stored\n",
    "BEST_MODEL_FILENAME = 'models/best_transfer_learning.hd5' # output model filename\n",
    "\n",
    "# deep neural network architecture:\n",
    "HIDDEN_LAYER_SIZE = 32 # number of units in the last dense layer\n",
    "OUTPUT_LAYER_SIZE = 2 # number of classes to predict (here \"non-tumoral\" and \"tumoral\")\n",
    "RESNET_POOLING = 'avg' # ['avg' | 'max' ]\n",
    "\n",
    "# training procedure hyperparameters:\n",
    "VALIDATION_SPLIT = 0.2 # proportion of images to be reserved for the validation dataset\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 10\n",
    "STEPS_PER_EPOCH_TRAINING = 1000\n",
    "STEPS_PER_EPOCH_VALIDATION = 10\n",
    "LOSS_FUNCTION = 'categorical_crossentropy'\n",
    "METRICS = ['accuracy']\n",
    "\n",
    "# optimizer hyperparameters:\n",
    "LEARNING_RATE = 0.01\n",
    "DECAY = 1e-6\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "# others:\n",
    "SEED = 42 # seed number for reproducibility of the training validation splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "We want to use a pretrained version of ResNet50, on the ImageNet dataset, and fine-tune it in order to classify tumoral from non-tumoral tiles. To do so, we add fully-connected (FC or Dense) layers and train them on our annotated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "\u001b[33mwarning: From /home/jeremy_damien_guillon/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                65568     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 23,653,346\n",
      "Trainable params: 65,634\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        ResNet50(weights='imagenet', include_top=False, input_shape=(HEIGHT, WIDTH, CHANNELS), pooling=RESNET_POOLING),\n",
    "        Dense(HIDDEN_LAYER_SIZE, activation='relu'),\n",
    "        Dense(OUTPUT_LAYER_SIZE, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # we do not train the ResNet50 layer since it is already pre-trained on the ImageNet dataset\n",
    "    model.layers[0].trainable = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image generator\n",
    "\n",
    "Since our dataset contains \"only\" arround 10k images, we might need to *augment* it, that's **data augmentation**. It consists, in our case, in rotating or flipping the tiles to make as if they were different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annot_tiles_filename = DATA_DIR + \"/train_input\" + \"/train_tile_annotations.csv\"\n",
    "logging.debug(f'reading csv file: {annot_tiles_filename}')\n",
    "annot_tiles_df = pd.read_csv(annot_tiles_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the `csv` file content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_387_annotated_tile_0_15_69_30.jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_387_annotated_tile_1_15_23_53.jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_387_annotated_tile_2_15_58_20.jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_387_annotated_tile_3_15_67_12.jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_387_annotated_tile_4_15_57_20.jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Unnamed: 0  Target\n",
       "0  ID_387_annotated_tile_0_15_69_30.jpg     0.0\n",
       "1  ID_387_annotated_tile_1_15_23_53.jpg     0.0\n",
       "2  ID_387_annotated_tile_2_15_58_20.jpg     0.0\n",
       "3  ID_387_annotated_tile_3_15_67_12.jpg     0.0\n",
       "4  ID_387_annotated_tile_4_15_57_20.jpg     0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_tiles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to convert the `Target` column to `str` for the `ImageDataGenerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_tiles_df['Target'] = annot_tiles_df['Target'].apply(lambda x: str(x))\n",
    "# annot_tiles_df['Unnamed: 0'] = annot_tiles_df['Unnamed: 0'].apply(lambda x: x[:6]+'/'+x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an image generator that will load on-the-fly batches of images in memory; raw and/or augmented (i.e. artifical) ones. The dataset will be randomly split in training and validation sets according to the `VALIDATION_SPLIT` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6607 images belonging to 2 classes.\n",
      "Found 1651 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "data_generator =  ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    rotation_range=90,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "training_generator = data_generator.flow_from_dataframe(\n",
    "    subset='training', # set it as the training dataset\n",
    "    dataframe=annot_tiles_df, x_col='Unnamed: 0', y_col='Target',\n",
    "    directory=IMAGES_DIR, # images directory where are stored files whose filenames are listed in the `x_col` of the dataframe\n",
    "    seed=SEED,\n",
    "    target_size=(HEIGHT, WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = data_generator.flow_from_dataframe(\n",
    "    subset='validation', # set it as the validation dataset\n",
    "    dataframe=annot_tiles_df, x_col='Unnamed: 0', y_col='Target',\n",
    "    directory=IMAGES_DIR, # images directory where are stored files whose filenames are listed in the `x_col` of the dataframe\n",
    "    seed=SEED,\n",
    "    target_size=(HEIGHT, WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random, string\n",
    "\n",
    "# define roc_callback, inspired by https://github.com/keras-team/keras/issues/6050#issuecomment-329996505\n",
    "def auc_roc(y_true, y_pred):\n",
    "    # any tensorflow metric\n",
    "    value, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)\n",
    "\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
    "\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "        return value\n",
    "    \n",
    "def random_id(length=4):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mwarning: From <ipython-input-8-d267efe9091c>:7: streaming_auc (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.metrics.auc. Note that the order of the labels and predictions arguments has been switched.\u001b[0m\n",
      "\u001b[33mwarning: From /home/jeremy_damien_guillon/.local/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py:526: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\u001b[0m\n",
      "\u001b[33mwarning: From /home/jeremy_damien_guillon/.local/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py:788: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "sgd = SGD(lr=LEARNING_RATE, decay=DECAY, momentum=MOMENTUM, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss=LOSS_FUNCTION, metrics=['accuracy', auc_roc])\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "checkpoint_cb = ModelCheckpoint(filepath=BEST_MODEL_FILENAME, \n",
    "                                monitor='val_auc_roc', \n",
    "                                save_best_only=True)\n",
    "tensorboard_cb = TensorBoard(log_dir=f'logs/{random_id()}', \n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             write_graph=True,\n",
    "                             update_freq='batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mwarning: From /home/jeremy_damien_guillon/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 509/1000 [==============>...............] - ETA: 26:09 - loss: 0.1088 - acc: 0.9745 - auc_roc: 0.9771"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    training_generator,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH_TRAINING,\n",
    "    epochs = NUM_EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=STEPS_PER_EPOCH_VALIDATION,\n",
    "    callbacks=[checkpoint_cb, tensorboard_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(BEST_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51772 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "test_data_generator =  ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "TEST_IMAGES_DIR = 'data/test_input'\n",
    "test_generator = test_data_generator.flow_from_directory(TEST_IMAGES_DIR,\n",
    "                                                    target_size=(HEIGHT, WIDTH),\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    shuffle=False,\n",
    "                                                    class_mode=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 34s 3s/step\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict_generator(test_generator, steps=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.93449628e-01, 6.55034836e-03],\n",
       "       [9.98783410e-01, 1.21659075e-03],\n",
       "       [9.86834586e-01, 1.31654376e-02],\n",
       "       [9.96193886e-01, 3.80612840e-03],\n",
       "       [9.89546120e-01, 1.04538472e-02],\n",
       "       [9.82967377e-01, 1.70326997e-02],\n",
       "       [9.75464821e-01, 2.45352443e-02],\n",
       "       [9.79764879e-01, 2.02351399e-02],\n",
       "       [9.64974940e-01, 3.50250416e-02],\n",
       "       [9.86985743e-01, 1.30142840e-02],\n",
       "       [9.67546642e-01, 3.24533433e-02],\n",
       "       [9.73244905e-01, 2.67551243e-02],\n",
       "       [9.33049142e-01, 6.69509098e-02],\n",
       "       [9.99437511e-01, 5.62413363e-04],\n",
       "       [9.94153798e-01, 5.84619027e-03],\n",
       "       [9.30909395e-01, 6.90905973e-02],\n",
       "       [9.73405123e-01, 2.65948549e-02],\n",
       "       [9.68381107e-01, 3.16189565e-02],\n",
       "       [9.73255038e-01, 2.67449953e-02],\n",
       "       [9.80822027e-01, 1.91780049e-02],\n",
       "       [9.95320141e-01, 4.67987871e-03],\n",
       "       [9.26325381e-01, 7.36746490e-02],\n",
       "       [9.55301762e-01, 4.46982682e-02],\n",
       "       [9.88095284e-01, 1.19047537e-02],\n",
       "       [9.97169912e-01, 2.83005252e-03],\n",
       "       [9.17160928e-01, 8.28391090e-02],\n",
       "       [9.82859671e-01, 1.71403531e-02],\n",
       "       [8.83109987e-01, 1.16890043e-01],\n",
       "       [9.89699185e-01, 1.03008747e-02],\n",
       "       [9.86833751e-01, 1.31662870e-02],\n",
       "       [9.89012122e-01, 1.09878182e-02],\n",
       "       [9.56223905e-01, 4.37761098e-02],\n",
       "       [9.84134614e-01, 1.58653911e-02],\n",
       "       [9.25127745e-01, 7.48722404e-02],\n",
       "       [9.87120688e-01, 1.28793065e-02],\n",
       "       [9.86183047e-01, 1.38169248e-02],\n",
       "       [9.99344885e-01, 6.55105105e-04],\n",
       "       [9.72309172e-01, 2.76908036e-02],\n",
       "       [9.68457997e-01, 3.15420330e-02],\n",
       "       [9.79878426e-01, 2.01215651e-02],\n",
       "       [9.74502802e-01, 2.54972242e-02],\n",
       "       [9.56994474e-01, 4.30054776e-02],\n",
       "       [9.95991051e-01, 4.00891434e-03],\n",
       "       [9.92705047e-01, 7.29492446e-03],\n",
       "       [9.14772570e-01, 8.52274820e-02],\n",
       "       [9.80078340e-01, 1.99217033e-02],\n",
       "       [9.87282276e-01, 1.27176652e-02],\n",
       "       [9.92499530e-01, 7.50052929e-03],\n",
       "       [9.90383446e-01, 9.61658917e-03],\n",
       "       [9.84305203e-01, 1.56948604e-02],\n",
       "       [9.84411240e-01, 1.55888358e-02],\n",
       "       [9.85795498e-01, 1.42045338e-02],\n",
       "       [9.97036457e-01, 2.96355155e-03],\n",
       "       [9.98891771e-01, 1.10825768e-03],\n",
       "       [9.75640893e-01, 2.43591517e-02],\n",
       "       [9.74438429e-01, 2.55615357e-02],\n",
       "       [9.59747732e-01, 4.02522720e-02],\n",
       "       [9.87065673e-01, 1.29343830e-02],\n",
       "       [9.79431093e-01, 2.05689352e-02],\n",
       "       [9.76162910e-01, 2.38371324e-02],\n",
       "       [9.49704111e-01, 5.02959602e-02],\n",
       "       [9.89002824e-01, 1.09971492e-02],\n",
       "       [9.96782899e-01, 3.21712485e-03],\n",
       "       [9.73019242e-01, 2.69808155e-02],\n",
       "       [9.16446328e-01, 8.35536942e-02],\n",
       "       [9.90480602e-01, 9.51942336e-03],\n",
       "       [9.86240447e-01, 1.37595152e-02],\n",
       "       [9.87674654e-01, 1.23253865e-02],\n",
       "       [9.81791794e-01, 1.82081461e-02],\n",
       "       [9.91190076e-01, 8.80998466e-03],\n",
       "       [9.63895738e-01, 3.61043103e-02],\n",
       "       [9.90277588e-01, 9.72240977e-03],\n",
       "       [9.50772285e-01, 4.92276475e-02],\n",
       "       [9.71994698e-01, 2.80052405e-02],\n",
       "       [9.62094188e-01, 3.79058346e-02],\n",
       "       [9.76085186e-01, 2.39148177e-02],\n",
       "       [9.91848826e-01, 8.15118104e-03],\n",
       "       [9.74854827e-01, 2.51451805e-02],\n",
       "       [9.75773096e-01, 2.42268834e-02],\n",
       "       [9.78050292e-01, 2.19496973e-02]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's left to do\n",
    "\n",
    "- [ ] Optimize, optimize, optimize the hyperparameters !\n",
    "- [ ] Predict tiles label on test data using a DataFrame to keep track of the subjects IDs\n",
    "- [ ] Infer weak labels of test subjects\n",
    "- [ ]Â Submit the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
